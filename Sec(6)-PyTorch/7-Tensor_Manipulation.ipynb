{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoNS1p9D11QS"
   },
   "source": [
    "# Tensor Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-gYNhhT-LCk"
   },
   "source": [
    "- reshaping\n",
    "- slicing\n",
    "- joining or splitting\n",
    "- transposing and permuting dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4507,
     "status": "ok",
     "timestamp": 1737649477284,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "qJKvbOR_-eBG"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737649595479,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "OXUp-4H7-sJf",
    "outputId": "0eae7bbc-8c01-4600-939e-705ea03e8fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "12\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# reshaping tensors\n",
    "# reshape\n",
    "# view\n",
    "\n",
    "original_tensor = torch.arange(12)\n",
    "print(original_tensor)\n",
    "print(original_tensor.nelement())\n",
    "print(original_tensor.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 440,
     "status": "ok",
     "timestamp": 1737649677703,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "EHcuth6C-20P",
    "outputId": "7bf3d67b-9db7-4782-cc79-37afdd68415c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n",
      "2\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# 2x6--> 12\n",
    "\n",
    "reshaped_tensor = original_tensor.reshape(2,6)\n",
    "print(reshaped_tensor)\n",
    "print(reshaped_tensor.ndim)\n",
    "print(reshaped_tensor.nelement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1737649719920,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "MD-OFtPf_Y_h",
    "outputId": "e88e1116-09ba-4c9d-afbb-16eb653da682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "2\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "reshaped_tensor = original_tensor.reshape(3,4)\n",
    "print(reshaped_tensor)\n",
    "print(reshaped_tensor.ndim)\n",
    "print(reshaped_tensor.nelement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance: Some PyTorch operations, especially those that interact directly with low-level memory (like certain CUDNN operations), expect contiguous tensors for optimal performance.\n",
    "# Compatibility: Certain functions or modules might require contiguous inputs to work correctly. If you encounter errors related to non-contiguous tensors, using contiguous() can resolve them.\n",
    "# View Operations: Operations like view() can only be applied to contiguous tensors. If you try to call view() on a non-contiguous tensor, you'll get an error, and you'll need to call .contiguous() first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 753,
     "status": "ok",
     "timestamp": 1737649893473,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "b3VvV1U__oYp",
    "outputId": "da34facf-1afa-484d-93e7-4b6d8b0910f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# view\n",
    "print(original_tensor.is_contiguous())\n",
    "flattened_tensor = original_tensor.view(-1)\n",
    "print(flattened_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfSq9ygNAXqU"
   },
   "source": [
    "## Slicing\n",
    "Extract specific portions of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1737650019918,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "xQj-NNdkAHPb",
    "outputId": "d0aada9e-3968-4374-d341-ae265ff76bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(tensor_a)\n",
    "print(tensor_a.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1737650046425,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "bXEwDf72AxSs",
    "outputId": "9d9c0114-8b74-4f6a-e233-883559079462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737650103677,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "9au3kmhQA38c",
    "outputId": "52d61f2f-c9be-435b-d457-5bb0e19bef45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 6, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1737650164219,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "VtbAKD9sA9i0",
    "outputId": "fcc14485-5d30-45fc-cef3-cf2638503143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1737650235451,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "Ms9vb0Z3BUx2",
    "outputId": "a0ecfc27-5cc4-40f0-b9bb-19cb430f2085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[5, 6],\n",
      "        [8, 9]])\n"
     ]
    }
   ],
   "source": [
    "sub_tensor = tensor_a[1:,1:]\n",
    "print(sub_tensor.shape)\n",
    "print(sub_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JOcRbJlB009"
   },
   "source": [
    "## Joining tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jis bracket/dimension ko concat krenge uske andr elements bdhege"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1914,
     "status": "ok",
     "timestamp": 1737650506328,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "JAh_aEHcBjxm",
    "outputId": "00ba4499-7bf7-4fe4-cab9-026bbf0bf5e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "====================\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat() --> merges tensors along an exisiting dimesion.\n",
    "tensor1 = torch.tensor([[1,2],[3,4]])\n",
    "tensor2 = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "print(tensor1)\n",
    "print(tensor2)\n",
    "\n",
    "print(\"=\"*20)\n",
    "\n",
    "concat_tensor_rows = torch.cat((tensor1, tensor2), dim=0)\n",
    "concat_tensor_colm = torch.cat((tensor1, tensor2), dim=1)\n",
    "\n",
    "print(concat_tensor_rows)\n",
    "print(concat_tensor_colm)\n",
    "# Possible range is dim range\n",
    "# In the same dimnsion clubs the element or increases the number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 684,
     "status": "ok",
     "timestamp": 1737650791323,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "ossnj2LlB_pv",
    "outputId": "6cf8ee3d-02ce-42ff-9a47-763635293a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "====================\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "tensor([[[1, 2],\n",
      "         [5, 6]],\n",
      "\n",
      "        [[3, 4],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "# stack --> created a new dimension, increases the tensor's rank\n",
    "#  both  tensor dimension must match\n",
    "# jis dim ko stack krenge uske andr same indexes club hongi\n",
    "print(tensor1.shape)\n",
    "print(tensor2.shape)\n",
    "\n",
    "print(\"=\"*20)\n",
    "stack_tensor_rows = torch.stack((tensor1, tensor2), dim=0)\n",
    "stack_tensor_colm = torch.stack((tensor1, tensor2), dim=1)\n",
    "\n",
    "print(stack_tensor_rows.shape)\n",
    "print(stack_tensor_colm.shape)\n",
    "\n",
    "print(stack_tensor_rows)\n",
    "print(stack_tensor_colm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_tensor_new = torch.stack((tensor1, tensor2), dim=2)\n",
    "# This works along the scalars means inside the last bracket\n",
    "# Stack d0*d1*d2\n",
    "# Possiblities dim=0 to 3\n",
    "# dim=x\n",
    "# no of tensors stack (should have same shape)=n\n",
    "# new shape=d0*n*x*(x+1)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUb3rHr5G3Oj"
   },
   "source": [
    "## Splitting tensors\n",
    "- torch.chunk() --> divides your tensor into equal-sized chunks\n",
    "- torch.split() --> allows uneven splitting based on size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1737651812420,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "VEWenDc_FSWI",
    "outputId": "0f2b2bf5-043a-469d-9be3-05b3a3f6f52d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([ 9, 10, 11]))\n",
      "tensor([0, 1, 2])\n",
      "tensor([3, 4, 5])\n",
      "tensor([6, 7, 8])\n",
      "tensor([ 9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# torch.chunk\n",
    "# This function may return fewer than the specified number of chunks!\n",
    "\n",
    "origin_tensor = torch.arange(12)\n",
    "# split into a number of chunks who divides the the number of elements in that dimension\n",
    "chunks = torch.chunk(origin_tensor, 5, dim=0)\n",
    "\n",
    "print(chunks)\n",
    "\n",
    "for chunk in chunks:\n",
    "  print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1695,
     "status": "ok",
     "timestamp": 1737651930416,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "zH8hb1nyHVel",
    "outputId": "d23a49ba-7130-4045-ae2f-1450dd890cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]), tensor([10, 11]))\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([10, 11])\n"
     ]
    }
   ],
   "source": [
    "# torch.split\n",
    "\n",
    "splits = torch.split(origin_tensor, 5, dim=0)\n",
    "# Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size.\n",
    "print(splits)\n",
    "\n",
    "for split in splits:\n",
    "  print(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7s5eKYnILUr"
   },
   "source": [
    "# Transposing and premuting\n",
    "\n",
    "- transpose() -->swaps two dimesnion. mxn --> nxm\n",
    "- premute() --> rearranges all dimension in the specified order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1737652164943,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "XK7389QnH7LG",
    "outputId": "79fc41ed-59fc-42c8-cec3-e2ecde583fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1],\n",
      "        [ 2,  3],\n",
      "        [ 4,  5],\n",
      "        [ 6,  7],\n",
      "        [ 8,  9],\n",
      "        [10, 11],\n",
      "        [12, 13],\n",
      "        [14, 15],\n",
      "        [16, 17],\n",
      "        [18, 19],\n",
      "        [20, 21],\n",
      "        [22, 23]])\n",
      "torch.Size([12, 2])\n",
      "torch.Size([2, 12])\n",
      "tensor([[ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22],\n",
      "        [ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23]])\n"
     ]
    }
   ],
   "source": [
    "# transpose\n",
    "\n",
    "tensor_original = torch.arange(24).reshape(12,2)\n",
    "\n",
    "transposed_tensor = tensor_original.transpose(0,1)\n",
    "\n",
    "print(tensor_original)\n",
    "\n",
    "print(tensor_original.shape)\n",
    "print(transposed_tensor.shape)\n",
    "\n",
    "print(transposed_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1737652532005,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "0RiEff8MI6xI",
    "outputId": "7b932f68-d071-46f9-d7af-292683ad1bf0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tensor_original = \u001b[43mtorch\u001b[49m.arange(\u001b[32m24\u001b[39m).reshape(\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m4\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[32m2\u001b[39m*\u001b[32m3\u001b[39m*\u001b[32m4\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(tensor_original.shape)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "tensor_original = torch.arange(24).reshape(2,3,4)\n",
    "\n",
    "print(2*3*4)\n",
    "\n",
    "print(tensor_original.shape)\n",
    "print(tensor_original)\n",
    "\n",
    "# [0,1,2]\n",
    "permuted_tensor = tensor_original.permute(2, 0, 1)\n",
    "print(permuted_tensor)\n",
    "print(permuted_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlQA8UXIJP3p"
   },
   "outputs": [],
   "source": [
    "l = [4,0,1,2,3,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcvN3zwzLQwj"
   },
   "source": [
    "# Cloning and detaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737652927073,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "SCxO9JcgLTam",
    "outputId": "a71e9e2a-d23a-419f-9ee0-71e9d219c1a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "a = [1,2,3]\n",
    "b = copy.deepcopy(a)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1737653089340,
     "user": {
      "displayName": "nimoy",
      "userId": "12145814232676261569"
     },
     "user_tz": -330
    },
    "id": "x7WeZ0FdLyhI"
   },
   "outputs": [],
   "source": [
    "tensor = torch.ones(3,3, requires_grad=True) # part of computation graph\n",
    "\n",
    "cloned_tensor = tensor.clone() # # part of computation graph\n",
    "\n",
    "detached_tensor = tensor.detach() # this code will detach the tensor from the computation graph\n",
    "# # non a part of computation graph\n",
    "# but storage will be same as the original ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HH6gZFSuMfAZ"
   },
   "outputs": [],
   "source": [
    "# | Tensor            | Tracks Gradients? | Part of Computation Graph? | Shares storage with original? |\n",
    "# | ----------------- | ----------------- | -------------------------- | ----------------------------- |\n",
    "# | `tensor`          | ✅ Yes             | ✅ Yes                      | Original                      |\n",
    "# | `cloned_tensor`   | ✅ Yes             | ✅ Yes                      | ❌ No (new memory)             |\n",
    "# | `detached_tensor` | ❌ No              | ❌ No                       | ✅ Yes (same memory)           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ What is a Computation Graph (in PyTorch and Deep Learning)?\n",
    "# A computation graph is like a dynamic map of all tensor operations performed on tensors that have requires_grad=True.\n",
    "\n",
    "# It tracks what operations happened, on which tensors, and in what order, so that during backpropagation, PyTorch knows how to compute gradients automatically.\n",
    "\n",
    "# ✅ Visual Example:\n",
    "# For this simple code:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# import torch\n",
    "\n",
    "# x = torch.tensor(2.0, requires_grad=True)\n",
    "# y = x * 3\n",
    "# z = y ** 2\n",
    "# PyTorch internally builds this computation graph:\n",
    "\n",
    "# scss\n",
    "# Copy\n",
    "# Edit\n",
    "# x --(multiply by 3)--> y --(square)--> z\n",
    "# So when you do:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# z.backward()\n",
    "# PyTorch walks backwards along this graph, calculates:\n",
    "\n",
    "# ∂\n",
    "# 𝑧\n",
    "# ∂\n",
    "# 𝑥\n",
    "# ∂x\n",
    "# ∂z\n",
    "# ​\n",
    " \n",
    "# ✅ Key Properties of Computation Graph:\n",
    "# Property\tExplanation\n",
    "# Dynamic in PyTorch\tThe graph is created at runtime (eager execution). Not static like TensorFlow 1.x\n",
    "# Nodes\tEach tensor and operation is a node\n",
    "# Edges\tEach operation applied is an edge connecting tensors\n",
    "# Direction\tIt flows forward during computation, and backward during .backward() call\n",
    "# Autograd system\tPyTorch automatically handles graph creation and gradient calculation\n",
    "\n",
    "# ✅ Why does PyTorch need this graph?\n",
    "# So that it can apply chain rule of calculus (backpropagation) and compute gradients automatically for each parameter you want to optimize (like weights in neural nets).\n",
    "\n",
    "# ✅ How Tensors relate to the graph:\n",
    "# Tensor\tPart of Graph?\n",
    "# Created with requires_grad=True\t✅ YES\n",
    "# Any tensor resulting from ops on it\t✅ YES (until detached)\n",
    "# Detached tensors\t❌ Not part of graph\n",
    "\n",
    "# ✅ Simple working example:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# x = torch.tensor(2.0, requires_grad=True)\n",
    "# y = x * 3\n",
    "# z = y ** 2\n",
    "\n",
    "# z.backward()\n",
    "\n",
    "# print(x.grad)   # Should print 12.0\n",
    "# Why?\n",
    "\n",
    "# Because:\n",
    "\n",
    "# bash\n",
    "# Copy\n",
    "# Edit\n",
    "# y = 3x → dy/dx = 3\n",
    "# z = y^2 → dz/dy = 2y = 2*6=12\n",
    "# dz/dx = dz/dy * dy/dx = 12\n",
    "# ✅ Real-life analogy:\n",
    "# Think of the computation graph like a \"history recorder\" of your tensor calculations, so PyTorch knows \"how you got this result\", and can \"reverse engineer\" gradients when training.\n",
    "\n",
    "# If you want, I can show how detaching breaks the graph in this example. Want me to?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# You said:\n",
    "# why they are required\n",
    "\n",
    "\n",
    "# ChatGPT said:\n",
    "# ✅ Why Computation Graphs are Required (In Deep Learning / PyTorch)?\n",
    "# 🎯 Main Reason: Automatic Gradient Calculation (Autograd)\n",
    "# Deep learning training is all about minimizing a loss function using gradient-based optimization algorithms like SGD, Adam, etc.\n",
    "\n",
    "# To do that, you need to compute gradients of the loss with respect to each model parameter (weights, biases, etc).\n",
    "\n",
    "# ✅ The Chain Rule Problem:\n",
    "# For even a small neural network:\n",
    "\n",
    "# css\n",
    "# Copy\n",
    "# Edit\n",
    "# Input → Linear → ReLU → Linear → Sigmoid → Loss\n",
    "# The math behind gradient calculation involves multiple nested chain rule steps, like:\n",
    "\n",
    "# ∂\n",
    "# 𝐿\n",
    "# ∂\n",
    "# 𝑤\n",
    "# ∂w\n",
    "# ∂L\n",
    "# ​\n",
    " \n",
    "# Where L = Loss, and w = any weight.\n",
    "\n",
    "# Manually writing these gradients for every layer, activation, etc., is:\n",
    "\n",
    "# ❌ Time-consuming\n",
    "\n",
    "# ❌ Error-prone\n",
    "\n",
    "# ❌ Unscalable (imagine 100-layer networks)\n",
    "\n",
    "# ✅ Computation Graph Solves This:\n",
    "# PyTorch automatically builds the graph of operations during forward pass.\n",
    "\n",
    "# So during .backward():\n",
    "\n",
    "# PyTorch traverses the graph backward\n",
    "\n",
    "# Applies chain rule automatically\n",
    "\n",
    "# Calculates gradients for all tensors that require gradients\n",
    "\n",
    "# ✅ Example Workflow:\n",
    "# Phase\tWhat happens\n",
    "# Forward Pass\tPyTorch records the operations and builds the computation graph\n",
    "# Loss Calculation\tOutput → Loss\n",
    "# Backward Pass\t.backward() triggers graph traversal for gradient computation\n",
    "# Optimizer Step\tGradients are used by optimizer to update weights\n",
    "\n",
    "# ✅ Without Computation Graph:\n",
    "# You'd have to manually calculate derivatives for each operation, like:\n",
    "\n",
    "# For a simple function:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# f(x) = (x * 3) ^ 2\n",
    "# You'd have to manually write:\n",
    "\n",
    "# 𝑑\n",
    "# 𝑓\n",
    "# 𝑑\n",
    "# 𝑥\n",
    "# =\n",
    "# 2\n",
    "# ×\n",
    "# (\n",
    "# 3\n",
    "# 𝑥\n",
    "# )\n",
    "# ×\n",
    "# 3\n",
    "# =\n",
    "# 18\n",
    "# 𝑥\n",
    "# dx\n",
    "# df\n",
    "# ​\n",
    "#  =2×(3x)×3=18x\n",
    "# Imagine doing this for big neural nets… Impossible at scale.\n",
    "\n",
    "# ✅ Benefits of Computation Graphs:\n",
    "# Benefit\tExplanation\n",
    "# No manual gradient calculation\tPyTorch does it\n",
    "# Works for any arbitrary operation\tEven your custom functions\n",
    "# Supports dynamic graphs\tPyTorch graphs are built at runtime (eager execution)\n",
    "# Memory-efficient\tPyTorch stores just enough history to compute gradients\n",
    "\n",
    "# ✅ Quick Recap:\n",
    "# Computation graphs are required because they enable automatic differentiation, which is fundamental for training neural networks using backpropagation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNPUTxWWGANscL5J/emY7g6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TorchLearnEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
